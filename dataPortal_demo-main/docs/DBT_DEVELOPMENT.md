# dbt Project Development Guide

## Where to Make Changes

### SQL Model Files

Edit SQL files in the following locations:

```
airflow_dbt/dbt/jaffle_shop/
├── models/
│   ├── customers.sql          # Main models
│   ├── orders.sql
│   ├── schema.yml             # Model documentation and tests
│   └── staging/
│       ├── stg_customers.sql  # Staging models
│       ├── stg_orders.sql
│       └── stg_payments.sql
├── seeds/
│   └── raw_customers.csv      # Seed data files
└── dbt_project.yml            # dbt project configuration
```

### File Locations on Your System

**Full path**: `/home/ubuntu/dataPortal_demo/demoDBTProject/airflow_dbt/dbt/jaffle_shop/`

**Common files to edit:**
- Models: `dbt/jaffle_shop/models/*.sql`
- Staging models: `dbt/jaffle_shop/models/staging/*.sql`
- Seeds: `dbt/jaffle_shop/seeds/*.csv`
- Configuration: `dbt/jaffle_shop/dbt_project.yml`
- Schema/tests: `dbt/jaffle_shop/models/schema.yml`

## Automatic Updates

### ✅ Changes Are Automatically Reflected

Since the `dbt` directory is mounted as a **Docker volume**, changes you make to files are **immediately available** in the Airflow containers:

```yaml
volumes:
  - ./dbt:/usr/local/airflow/dbt  # Host directory → Container directory
```

**What this means:**
- ✅ Edit SQL files on your host machine → Changes are instantly visible in containers
- ✅ No need to rebuild Docker images
- ✅ No need to restart containers (for dbt files)
- ✅ Changes are picked up on the next DAG run

## Workflow After Making Changes

### 1. Edit SQL Files

Edit any `.sql` file in the `dbt/jaffle_shop/models/` directory:

```bash
# Example: Edit a model
nano dbt/jaffle_shop/models/customers.sql
# or
vim dbt/jaffle_shop/models/staging/stg_customers.sql
```

### 2. Verify Changes Are Visible

Check that the file is updated in the container:

```bash
# View file in container
sudo docker exec airflow_dbt_airflow-scheduler_1 \
  cat /usr/local/airflow/dbt/jaffle_shop/models/customers.sql
```

### 3. Run the DAG

**Option A: Via Airflow UI**
1. Go to https://airflow.portal.getzingle.com
2. Find `dbt_jaffle_shop` DAG
3. Click "Play" button to trigger manually
4. Monitor execution

**Option B: Via API**
```bash
curl -X POST \
  -u airflow:airflow \
  -H "Content-Type: application/json" \
  -d '{"conf": {}}' \
  https://airflow.portal.getzingle.com/api/v1/dags/dbt_jaffle_shop/dagRuns
```

### 4. View Results

- Check Airflow UI for task logs
- Check Snowflake for updated tables
- Review dbt logs in: `airflow_dbt/logs/`

## What Happens on DAG Run

When you trigger the DAG, it will:

1. **generate_dbt_profiles**: Creates/updates `profiles.yml` (uses latest env vars)
2. **dbt_seed**: Loads seed data (if you changed `.csv` files)
3. **dbt_run**: Executes all SQL models (including your changes)
4. **dbt_test**: Runs tests defined in `schema.yml`

## Important Notes

### ⚠️ DAG File Changes

If you change the **DAG file** (`dags/dbt_jaffle_shop_dag.py`):
- Airflow scheduler auto-reloads DAGs every 30-60 seconds
- Or restart scheduler: `sudo docker-compose restart airflow-scheduler`

### ⚠️ dbt_project.yml Changes

If you change `dbt_project.yml`:
- Changes are picked up automatically on next `dbt run`
- No container restart needed

### ⚠️ New Files

If you add **new SQL files**:
- Place them in `models/` or `models/staging/`
- They're automatically included in the next `dbt run`
- No configuration needed (unless you want to exclude them)

### ⚠️ Target Directory

The `target/` directory contains compiled SQL and should **not** be edited:
- It's generated by dbt
- Changes here will be overwritten
- Edit source files in `models/` instead

## Example: Updating a Model

### Step 1: Edit the SQL File

```bash
cd /home/ubuntu/dataPortal_demo/demoDBTProject/airflow_dbt
vim dbt/jaffle_shop/models/customers.sql
```

### Step 2: Verify Change

```bash
# Check the file was updated
cat dbt/jaffle_shop/models/customers.sql
```

### Step 3: Trigger DAG

Go to Airflow UI and click "Play" on `dbt_jaffle_shop` DAG.

### Step 4: Check Results

```bash
# View dbt logs
sudo docker exec airflow_dbt_airflow-scheduler_1 \
  tail -f /usr/local/airflow/logs/dag_id=dbt_jaffle_shop/*/task_id=dbt_run/*.log
```

## Testing Changes Locally (Optional)

You can test dbt changes locally before running in Airflow:

```bash
cd /home/ubuntu/dataPortal_demo/demoDBTProject/jaffle_shop

# Install dbt-snowflake
pip install dbt-snowflake

# Copy profiles
cp ../profiles.yml ~/.dbt/profiles.yml

# Test your changes
dbt run --select customers
dbt test --select customers
```

## Best Practices

1. **Version Control**: Commit changes to git before running in production
2. **Test Locally**: Test SQL changes locally if possible
3. **Incremental Changes**: Make small, incremental changes
4. **Monitor Logs**: Always check Airflow task logs after changes
5. **Backup**: Consider backing up Snowflake tables before major changes

## Troubleshooting

### Changes Not Reflecting

1. **Check file path**: Ensure you're editing files in `dbt/jaffle_shop/models/`
2. **Check volume mount**: Verify docker-compose.yml has the volume mount
3. **Check container**: `sudo docker exec airflow_dbt_airflow-scheduler_1 ls -la /usr/local/airflow/dbt/jaffle_shop/models/`

### DAG Not Picking Up Changes

1. **Wait 30-60 seconds**: Airflow auto-reloads DAGs periodically
2. **Restart scheduler**: `sudo docker-compose restart airflow-scheduler`
3. **Check DAG parsing**: Look for errors in Airflow UI → DAGs → dbt_jaffle_shop

### SQL Errors

1. **Check syntax**: Validate SQL syntax before running
2. **Check logs**: Review task logs in Airflow UI
3. **Test locally**: Run `dbt compile` or `dbt parse` locally first
